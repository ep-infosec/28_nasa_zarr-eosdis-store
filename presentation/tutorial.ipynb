{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOSDIS Zarr Store: Spatial & Variable Subsetting without Services\n",
    "\n",
    "## Goal\n",
    "\n",
    "Produce a library that would let the Zarr Python library read EOSDIS cloud holdings efficiently, without requiring any modifications to our archive.  This has the potential to expand use to new communities and tools, allow more efficient access both in place and outside of the cloud, and therefore save money for the archive as well as time for users.\n",
    "\n",
    "## Background\n",
    "\n",
    "This is a demo of a data store I've been working on, building off of the work of a few others.  Adapting the Zarr library (which is meant to read cloud-optimized data stores) read NetCDF4 / HDF 5 files was discussed as a possibility at Summer ESIP last year.  Rich Signell from USGS worked with HDF Group to get [a prototype](https://medium.com/pangeo/cloud-performant-reading-of-netcdf4-hdf5-data-using-the-zarr-library-1a95c5c92314).  The resulting code showed no performance degradation over an equivalent native Zarr store.  This adaptation requies an up-front generation of metadata containing data attributes and byte offsets to allow efficient reads.\n",
    "\n",
    "## What I did\n",
    "\n",
    "I recognized that the DMR++ files OPeNDAP / GHRC have started generating on ingest in PI 20.1 contain nearly equivalent information to that required by the Zarr library.  Hearing that small chunk sizes (chunks are a region of data that can / must be read all at once) caused issues for some NetCDF files and required re-chunking (i.e. altering the original data file), I further looked at mitigating that issue to avoid having to re-host data.  In picking through the Zarr code, I came across a for loop that, if changed, would allow a set of optimizations that would greatly improve performance.  I advocated for this in the Zarr tracker and what we need is now being planned.\n",
    "![png](summary.png)\n",
    "\n",
    "In terms of actual code, I produced a Python library, eosdis-zarr-store that:\n",
    "\n",
    "1. Implements the Zarr storage API in a natural and familiar way to Zarr developers\n",
    "2. Sets up HTTP access to allow EDL credential handshaking and, importantly, caching of redirect URLs\n",
    "3. Adapts our underlying data files and DMR++ files generated on ingest to a Zarr-compatible API\n",
    "4. Implements optimizations using the API worked out with the Zarr community to make fewer total data reads and do them in parallel where possible\n",
    "\n",
    "The remainder of this notebook contains results and conclusions.\n",
    "\n",
    "## How to use it\n",
    "\n",
    "In the eosdis-zarr-store directory run `pip install -e .`.  Obtain or stage an HDF5 (NetCDF4) file along with a DMR++ file with identical URL + \".dmrpp\", you can run \"mkdmrpp\" in this folder to produce DMR++ files.  Then:\n",
    "\n",
    "```python\n",
    "from eosdis_zarr_store import Store\n",
    "import zarr\n",
    "\n",
    "f = zarr.open(Store(data_file_url))\n",
    "# Manipulate f as any Zarr store (see examples below)\n",
    "```\n",
    "\n",
    "The URLs in this notebook will not be available for general use, since one example produces 500 MB of egress for benchmarking.\n",
    "\n",
    "## Helpers and Constants (You can skip this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to draw stuff, generate URLs, and translate bounding boxes to array indices, \n",
    "# wholly ignoring all the helpful attributes present in the HDF and Zarr metadata\n",
    "# Please don't judge me on this mess.  It's not called \"Clean code fest\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from ipypb import track\n",
    "import numpy as np\n",
    "\n",
    "def show(data, transpose=True):\n",
    "    plt.rcParams[\"figure.figsize\"] = [16, 8]\n",
    "    if transpose:\n",
    "        data = np.transpose(data)\n",
    "    plt.imshow(data[::-1,:], norm=colors.Normalize(0, 150), cmap='Blues')\n",
    "\n",
    "def get_aoi(bbox, scale, x0=180, y0=90):\n",
    "    aoi = (0, \n",
    "           slice(scale * int(bbox[1] + x0), scale * int(bbox[3] + x0) + 1), \n",
    "           slice(scale * int(bbox[0] +  y0), scale * int(bbox[2] + y0) + 1))\n",
    "    shape = [d.stop - d.start for d in aoi[1:]]\n",
    "    return aoi, shape\n",
    "\n",
    "url_root = 'https://harmony.uat.earthdata.nasa.gov/service-results/harmony-uat-staging/public/demo/zarr-store/'\n",
    "# GPM HHR URLs\n",
    "filename_template = '3B-HHR.MS.MRG.3IMERG.20051022-S%02d%02d00-E%02d%02d59.%04d.V06B.HDF5'\n",
    "data_urls = [ url_root + filename_template % (h, m, h, m + 29, h * 60 + m) for h in range(0, 24) for m in range(0, 60, 30) ]\n",
    "\n",
    "bbox = [10, -100, 47.5, -45]\n",
    "\n",
    "# Basic file info (also readable from metadata)\n",
    "GPM_NODATA = -9999.9\n",
    "gpm_aoi, gpm_shape = get_aoi(bbox, 10)\n",
    "RSS_NODATA = 251\n",
    "RSS_SCALE_FACTOR = 0.5 # In-file scale factor is 0.1.  This increases it solely for the purpose of making it show up in pics\n",
    "rss_aoi, rss_shape = get_aoi(bbox, 4, 360)\n",
    "rss_aoi = (0, rss_aoi[2], rss_aoi[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm_aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Atmospheric water vapor off the East Coast on Patrick's wedding day\n",
    "\n",
    "It rained a little that day in DC and hurricanes were threatening our honeymoon in the Carribbean.\n",
    "\n",
    "We have a bounding box defined above.  Use data distributed by GHRC derived from the SSMIS sensor of the F16 DMSP satellite to build a picture.\n",
    "\n",
    "### Without Partial Access\n",
    "\n",
    "Download 2.6 MB file and subset it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from h5py import File as H5File\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url_root + 'f16_ssmis_20051022v7.nc')\n",
    "with H5File(BytesIO(response.content), 'r') as f:\n",
    "    aoi_data = f['atmosphere_water_vapor_content'][rss_aoi]\n",
    "    show(np.where(aoi_data < RSS_NODATA, aoi_data * RSS_SCALE_FACTOR, 0), transpose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Access Step 1 - Make our data readable in Zarr for partial access\n",
    "\n",
    "Mimick a Zarr store by reading OPeNDAP's DMR++ files and returning their metadata in a Zarr interface.  DMR++ files are planned to be generated on ingest\n",
    "\n",
    "Downloads 200 KB of data from the 2.6 MB file with conventional Zarr storage implementation.\n",
    "\n",
    "Result: 12 data requests, each of which goes through internet services, pre-signs a URL, and redirects to the data range.  All sequentially.  Slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from eosdis_store import EosdisStore\n",
    "import zarr\n",
    "\n",
    "f = zarr.open(EosdisStore(url_root + 'f16_ssmis_20051022v7.nc'))\n",
    "aoi_data = f['atmosphere_water_vapor_content'][rss_aoi]\n",
    "print(aoi_data)\n",
    "show(np.where(aoi_data < RSS_NODATA, aoi_data * RSS_SCALE_FACTOR, 0), transpose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Access Step 2 - Make Zarr reads fast\n",
    "\n",
    "Downloads 200 KB of data from the 2.6 MB file with Zarr optimizations: \n",
    "1. (Working with Zarr community) Implement \"getitems\" concept, allowing storage to know all of the chunks that will be accessed up front\n",
    "2. Combine nearby range requests into single HTTP requests before sending them, allowing fewer requests.\n",
    "3. Cache presigned URLs returned by the archive for a short time, as directed by caching headers (TEA has a ticket to add these), allowing reuse and avoiding many round-trips and redirects\n",
    "4. Run the first data range request serially to get the presigned URL.  Run subsequent requests in parallel.\n",
    "\n",
    "Result: 3 data requests, one of which goes through internet services, pre-signs a URL, and redirects to the data range.  The following two reuse the signed URL and fetch in parallel.  Faster!\n",
    "\n",
    "When more than a couple of chunks are involved, this is expected to be faster than the native Zarr S3 format is capable of, and the more chunks involved in a read the more it improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from eosdis_store import EosdisStore\n",
    "import zarr\n",
    "\n",
    "f = zarr.open(EosdisStore(url_root + 'f16_ssmis_20051022v7.nc'))\n",
    "aoi_data = f['atmosphere_water_vapor_content'][rss_aoi]\n",
    "print(rss_aoi)\n",
    "show(np.where(aoi_data < RSS_NODATA, aoi_data * RSS_SCALE_FACTOR, 0), transpose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f['atmosphere_water_vapor_content'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Rain along the East Coast on Patrick's wedding day\n",
    "\n",
    "With the same bounding box above, get half-hourly high-quality precipitation values from GPM and sum them for the entire day (48 L3 global data files)\n",
    "\n",
    "### Without Partial Access\n",
    "\n",
    "Download approximately 500MB of data in whole files and processes them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from h5py import File as H5File\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "result = np.zeros(gpm_shape)\n",
    "for url in track(data_urls):\n",
    "    response = requests.get(url)\n",
    "    with H5File(BytesIO(response.content), 'r') as f:\n",
    "        aoi_data = f['Grid/HQprecipitation'][gpm_aoi]\n",
    "        result = result + np.where(aoi_data != GPM_NODATA, aoi_data / 2, 0)\n",
    "show(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the EOSDIS Zarr Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads approximately 5 MB of data by doing partial reads in fewer lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from eosdis_store import EosdisStore\n",
    "import zarr\n",
    "\n",
    "result = np.zeros(gpm_shape)\n",
    "for url in track(data_urls):\n",
    "    f = zarr.open(EosdisStore(url, quiet=True))\n",
    "    aoi_data = f['Grid/HQprecipitation'][gpm_aoi]\n",
    "    result = result + np.where(aoi_data != GPM_NODATA, aoi_data / 2, 0)\n",
    "show(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to L3 Daily Average Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a 30 MB file from the daily average collection to produce a similar result, validating the result at 6x egress cost of partial access for all of the half-hourly source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from h5py import File as H5File\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url_root + '3B-DAY.MS.MRG.3IMERG.20051022-S000000-E235959.V06.nc4')\n",
    "with H5File(BytesIO(response.content), 'r') as f:\n",
    "    show(f['HQprecipitation'][gpm_aoi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I can see my house from here!\n",
    "\n",
    "Download GEDI L2B data.  Use small geolocation arrays to find the area of interest, then download only the data within those chunks.  \n",
    "\n",
    "A full file download is 1.3 GB.  The code below downloads approximately 15 MB of data and metadata.  This reduces a 15 minute download to about 8s.  (Aside: the download is 2/3 metadata, which could be dramatically reduced by using Zarr's default format rather than DMR++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eosdis_zarr_store import Store\n",
    "import zarr\n",
    "import numpy as np\n",
    "\n",
    "url = 'http://localhost:4000/data/GEDI02_B_2019182140038_O03117_T05635_02_001_01.h5'\n",
    "f = zarr.open(Store(url))\n",
    "print(f.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, w, s, e = [40.2, -75.25, 40.15, -75.2]\n",
    "\n",
    "geoloc = f['BEAM0000/geolocation']\n",
    "all_lats = geoloc['latitude_bin0'][:]\n",
    "all_lons = geoloc['longitude_bin0'][:]\n",
    "valid_lat_i = np.where(np.logical_and(all_lats >= s, all_lats <= n))\n",
    "valid_lon_i = np.where(np.logical_and(all_lons >= w, all_lons <= e))\n",
    "indices = np.intersect1d(valid_lat_i, valid_lon_i)\n",
    "\n",
    "lats = all_lats[indices]\n",
    "lons = all_lons[indices]\n",
    "data = f['BEAM0000/cover'][:][indices]\n",
    "data_i = np.where(data != -9999)\n",
    "data = data[data_i]\n",
    "lats = lats[data_i]\n",
    "lons = lons[data_i]\n",
    "\n",
    "ambler = plt.imread('ambler.png')\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(lons, lats, s=50, c=data, cmap='Greens')\n",
    "ax.set_xlim(w, e)\n",
    "ax.set_ylim(s, n)\n",
    "ax.imshow(ambler, zorder=0, extent = [w, e, s, n], aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it sometimes slower?\n",
    "\n",
    "![png](request-overhead.png)\n",
    "\n",
    "We pay a penalty for every new file we access, needing to go over the Internet, through the Internet services stack, the request signing process, and ultimately get redirected to S3.  The Zarr store has to pay this penalty twice to read the metadata and then the file, while a full-file download only pays the penalty once.  With current performance, the break-even point in file size is about 10 MB.  That is to say, if a user wants to access even a tiny amount of data in each granule from a collection whose granules are under 10 MB in size, he or she is better off downloading the granules.  While there is some uncontrollable overhead, there is significant room for improvement in areas that are under our control to promote inexpensive access patterns while improving time to science.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "* If providers generate DMR++ on ingest, we can expose our data efficiently using a Python API that is gaining increasing traction, particulary in the Pangeo community, with minimal storage overhead\n",
    "* Works out of the cloud, but works even better / faster in the cloud for analysis near data\n",
    "* For partial access cases, an overall egress reduction of 90% or more could be possible, as demonstrated\n",
    "* Chunking matters.  This work makes smaller chunks more desirable, which has not historically been the case with Zarr\n",
    "* Overhead in our stack, from EDL, to Internet services, to redirects, are eating up the potential user savings.  At a 90% egress reduction, we struggle to compete with \"Just download everything.\"  How do we balance preventing undesirable behavior with encouraging desirable behavior?\n",
    "* There are lingering questions about whether DMR++ is the correct format to capture this metadata in.  Zarr's native format is in many cases more complete and easier to parse while having mechanisms for more easily working with the 100,000-ish chunks in GEDI granules and for unifying multiple granules into a coherent view.\n",
    "\n",
    "## Limitations / Needs\n",
    "\n",
    "* The DMR++ file must be generated on ingest into the cloud, which is currently optional\n",
    "* Only works on HDF5 and NetCDF4 files.  In principle, it could work on HDF4 / NetCDF Classic files but nothing yet generates the necessary metadata\n",
    "* DMR++ does not quite specify everything we could need for some datasets.  We assume little endian byte order and column-major ordering.\n",
    "\n",
    "## Future Work\n",
    "\n",
    "* Packaging, unit tests, and docs sufficient for publication\n",
    "* Open source (relies on a naming decision)\n",
    "* Cache repeated calls for the same byte ranges to avoid requerying data we have\n",
    "* Implement unknown / undocumented areas of the DMR++ spec, including compression types and data filters\n",
    "* Tests with Dask and XArray\n",
    "* Implement CF conventions to populate fill values, offsets, scales, etc\n",
    "* Extensions to present L3 global collections as a coherent data cube\n",
    "\n",
    "I strongly believe in this access pattern as a win for our users and ourselves.  To the extent it is not fully realized, it suffers from being an early adopter of our cloud access stack.  My sincere hope is that we can learn from it to improve partial file access not only here but in other tools and libraries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
